{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arshanrahman/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/arshanrahman/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import randint as sp_randint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCOLUMNS = [\\'gx\\',\\'gy\\',\\'gz\\', \\'tax\\', \\'tay\\', \\'taz\\']\\n\\nX_train = pd.DataFrame(COLUMNS)\\nfor i in range(0,6):\\n    X_train[i] = pd.read_csv(\\'data/train/\\'+INPUT_DATA[3+i]+\\'train.txt\\', sep=\" \", header=None)\\n    X_test[i] = pd.read_csv(\\'data/test/\\'+INPUT_DATA[3+i]+\\'test.txt\\', sep=\" \", header=None)\\n\\n\\n\\n#X_train = pd.read_csv(\\'data/X_train.txt\\', sep=\" \", header=None)\\nY_train = pd.read_csv(\\'data/Y_train.txt\\', sep=\" \", header=None)\\n\\n#X_test = pd.read_csv(\\'data/X_test.txt\\', sep=\" \", header=None)\\nY_test = pd.read_csv(\\'data/Y_test.txt\\', sep=\" \", header=None)\\n\\nprint(X_train.shape)\\nprint(Y_train.shape)\\nprint(X_test.shape)\\nprint(Y_test.shape)\\n\\nX_train.head()\\nY_train.head()\\n\\nX = X_train[0:1000]\\nY = Y_train[0:1000]\\nXt = X_test[0:300]\\nYt = Y_test[0:300]\\n\\nprint(X.shape)\\nprint(Y.shape)\\nprint(Xt.shape)\\nprint(Yt.shape)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DATA = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "TRAINING_SIZE = 1000\n",
    "TESTING_SIZE = 300\n",
    "\n",
    "'''\n",
    "COLUMNS = ['gx','gy','gz', 'tax', 'tay', 'taz']\n",
    "\n",
    "X_train = pd.DataFrame(COLUMNS)\n",
    "for i in range(0,6):\n",
    "    X_train[i] = pd.read_csv('data/train/'+INPUT_DATA[3+i]+'train.txt', sep=\" \", header=None)\n",
    "    X_test[i] = pd.read_csv('data/test/'+INPUT_DATA[3+i]+'test.txt', sep=\" \", header=None)\n",
    "\n",
    "\n",
    "\n",
    "#X_train = pd.read_csv('data/X_train.txt', sep=\" \", header=None)\n",
    "Y_train = pd.read_csv('data/Y_train.txt', sep=\" \", header=None)\n",
    "\n",
    "#X_test = pd.read_csv('data/X_test.txt', sep=\" \", header=None)\n",
    "Y_test = pd.read_csv('data/Y_test.txt', sep=\" \", header=None)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "X_train.head()\n",
    "Y_train.head()\n",
    "\n",
    "X = X_train[0:1000]\n",
    "Y = Y_train[0:1000]\n",
    "Xt = X_test[0:300]\n",
    "Yt = Y_test[0:300]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def feature_extraction(x, y, z):\n",
    "    features = [np.mean(x), np.mean(y), np.mean(z), np.std(x), np.std(y), np.std(z)]\n",
    "    #Median Absolute Deviation\n",
    "    features.extend((np.mean(abs(x - features[0])), np.mean(abs(y - features[1])), np.mean(abs(z - features[2]))))\n",
    "    #Jerk Signals\n",
    "    features.extend((np.mean(np.diff(x)), np.mean(np.diff(y)), np.mean(np.diff(z)), np.std(np.diff(x)), np.std(np.diff(y)), np.std(np.diff(z))))\n",
    "    features.extend((np.mean(abs(np.diff(x) - features[9])), np.mean(abs(np.diff(y) - features[10])), np.mean(abs(np.diff(y) - features[11]))))\n",
    "    #features.extend((max(x), max(y), max(z), min(x), min(y), min(z)))\n",
    "    return features\n",
    "\n",
    "def feature_selection(X):\n",
    "    data = []\n",
    "    for i in range(X.shape[0]):\n",
    "        features = []\n",
    "        for j in range(0, X.shape[2], 3):\n",
    "            x = [X[i][u][j] for u in range(X.shape[1])]\n",
    "            y = [X[i][u][j+1] for u in range(X.shape[1])]\n",
    "            z = [X[i][u][j+2] for u in range(X.shape[1])]\n",
    "            features.append(feature_extraction(x, y, z))\n",
    "        data.append(normalize(features))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9)\n",
      "(7352,)\n",
      "(2947, 128, 9)\n",
      "(2947,)\n",
      "[[  1.80851493e-04   1.07668098e-02   5.55606782e-02 ...,   1.01281703e+00\n",
      "   -1.23216704e-01   1.02934100e-01]\n",
      " [  1.01385601e-02   6.57948013e-03   5.51248305e-02 ...,   1.02283299e+00\n",
      "   -1.26875594e-01   1.05687201e-01]\n",
      " [  9.27557424e-03   8.92887823e-03   4.84047309e-02 ...,   1.02202797e+00\n",
      "   -1.24003701e-01   1.02102503e-01]\n",
      " ..., \n",
      " [ -1.14748406e-03   1.71443899e-04   2.64786393e-03 ...,   1.01844501e+00\n",
      "   -1.24069601e-01   1.00385197e-01]\n",
      " [ -2.22265502e-04   1.57418102e-03   2.38105701e-03 ...,   1.01937199e+00\n",
      "   -1.22745097e-01   9.98735502e-02]\n",
      " [  1.57550001e-03   3.07018892e-03  -2.26975698e-03 ...,   1.02117097e+00\n",
      "   -1.21325999e-01   9.49874073e-02]]\n",
      "5\n",
      "(1000, 128, 9)\n",
      "(1000,)\n",
      "(300, 128, 9)\n",
      "(300,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f9acd0614685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-202a7144c311>\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in \n",
    "             [row.replace('  ', ' ').strip().split(' ') for row in file]\n",
    "            ]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    return y_\n",
    "\n",
    "\n",
    "X_train_input_paths = [\"data/train/\" + signal + \"train.txt\" for signal in INPUT_DATA]\n",
    "X_test_input_paths = [\"data/test/\" + signal + \"test.txt\" for signal in INPUT_DATA]\n",
    "X_train = load_X(X_train_input_paths)\n",
    "X_test = load_X(X_test_input_paths)\n",
    "\n",
    "y_train_path = \"data/y_train.txt\"\n",
    "y_test_path = \"data/y_test.txt\"\n",
    "Y_train = load_y(y_train_path).ravel()\n",
    "Y_test = load_y(y_test_path).ravel()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(Y_train[0])\n",
    "\n",
    "X = X_train[0:TRAINING_SIZE]\n",
    "Y = Y_train[0:TRAINING_SIZE]\n",
    "Xt = X_test[0:TESTING_SIZE]\n",
    "Yt = Y_test[0:TESTING_SIZE]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)\n",
    "\n",
    "X = np.array(feature_selection(X))\n",
    "Xt = np.array(feature_selection(Xt))\n",
    "\n",
    "print(X.shape)\n",
    "print(Xt.shape)\n",
    "\n",
    "print(X[0])\n",
    "#print(Xt[0])\n",
    "\n",
    "np.savetxt(\"data/test_data_format.csv\", X[0], fmt='%5s', delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "np.random.seed(121)\n",
    "C_range = np.random.normal(1, 0.2, 10).astype(float)\n",
    "\n",
    "# Check that C>0 \n",
    "C_range[C_range < 0] = 0.0001\n",
    "\n",
    "hyperparameters = {'penalty': penalty, \n",
    "                    'C': C_range}\n",
    "\n",
    "print (hyperparameters)\n",
    "\n",
    "clf = RandomizedSearchCV(LogisticRegression(), param_distributions=hyperparameters, cv=3, scoring='accuracy')\n",
    "clf.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "best_penalty = clf.best_params_['penalty']\n",
    "best_C = clf.best_params_['C']\n",
    "\n",
    "print (\"The best performing penalty is: {}\".format(best_penalty))\n",
    "print (\"The best performing C value is: {:5.2f}\".format(best_C))\n",
    "\n",
    "# examine the best model\n",
    "print(\"Best score\")\n",
    "print(clf.best_score_)\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(clf.grid_scores_)\n",
    "print()\n",
    "\n",
    "Y_pred = clf.predict(Xt.reshape(Xt.shape[0], Xt.shape[1]*Xt.shape[2]))\n",
    "print(\"Accuracy Rate:\")\n",
    "print(metrics.accuracy_score(Yt, Y_pred))\n",
    "print(metrics.confusion_matrix(Yt, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(Yt, Y_pred, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(Yt, Y_pred)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(LABELS))\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "\n",
    "\n",
    "# define the parameter values that should be searched\n",
    "k_range = list(range(1, 31))\n",
    "weight_options = ['uniform', 'distance']\n",
    "param_dist = dict(n_neighbors=k_range, weights=weight_options)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, param_dist, cv=4, scoring='accuracy')\n",
    "grid.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "#print(grid.grid_scores_)\n",
    "#grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "#print(grid_mean_scores)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "# plot the results\n",
    "#plt.plot(k_range, grid_mean_scores)\n",
    "#plt.xlabel('Value of K for KNN')\n",
    "#plt.ylabel('Cross-Validated Accuracy')\n",
    "\n",
    "\n",
    "rand = RandomizedSearchCV(knn, param_dist, cv=4, scoring='accuracy', n_iter=10, random_state=5)\n",
    "rand.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "#print(rand.grid_scores_)\n",
    "#rand_mean_scores = [result.mean_validation_score for result in rand.grid_scores_]\n",
    "#print(rand_mean_scores)\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)\n",
    "\n",
    "#plt.plot(k_range, rand_mean_scores)\n",
    "#plt.xlabel('Value of K for KNN')\n",
    "#plt.ylabel('Cross-Validated Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = grid.predict(Xt.reshape(Xt.shape[0], Xt.shape[1]*Xt.shape[2]))\n",
    "print(\"Accuracy Rate:\")\n",
    "print(metrics.accuracy_score(Yt, Y_pred))\n",
    "print(metrics.confusion_matrix(Yt, Y_pred))\n",
    "\n",
    "Y_pred1 = rand.predict(Xt.reshape(Xt.shape[0], Xt.shape[1]*Xt.shape[2]))\n",
    "print(\"Accuracy Rate:\")\n",
    "print(metrics.accuracy_score(Yt, Y_pred1))\n",
    "print(metrics.confusion_matrix(Yt, Y_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {}%\".format(100*metrics.precision_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(Yt, Y_pred, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(Yt, Y_pred)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(LABELS))\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "param_dist = {\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': np.logspace(-4, 2, 3),\n",
    "    'C': np.logspace(-4, 2, 5),\n",
    "}\n",
    "svm = SVC(probability=True)\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=svm,\n",
    "    param_distributions=param_dist,\n",
    "    cv=5,\n",
    ")\n",
    "clf.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "\n",
    "best_gamma  = clf.best_params_['gamma']\n",
    "best_C      = clf.best_params_['C']\n",
    "\n",
    "print(\"The best performing gamma value is: {:5.2f}\".format(best_gamma))\n",
    "print(\"The best performing C value is: {:5.2f}\".format(best_C))\n",
    "\n",
    "print(\"Best score\")\n",
    "print(clf.best_score_)\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(clf.grid_scores_)\n",
    "print()\n",
    "    \n",
    "svm = SVC(kernel='rbf', C=clf.best_params_['C'], gamma=clf.best_params_['gamma'])\n",
    "svm.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "Y_pred = svm.predict(Xt.reshape(Xt.shape[0], Xt.shape[1]*Xt.shape[2]))\n",
    "print(\"Accuracy Rate:\")\n",
    "print(metrics.accuracy_score(Yt, Y_pred))\n",
    "print(metrics.confusion_matrix(Yt, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {}%\".format(100*metrics.precision_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(Yt, Y_pred, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(Yt, Y_pred)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(LABELS))\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [20, 25, 30, 35],\n",
    "    \"max_features\": sp_randint(1, min(300, X.shape[1])),\n",
    "    \"min_samples_split\": sp_randint(2, 20),\n",
    "    \"min_samples_leaf\": sp_randint(2, 20),\n",
    "    'n_estimators': [50, 100, 200, 250, 300],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "r_forest = RandomForestClassifier(random_state=1)\n",
    "n_iter_search = 15\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=r_forest,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search,\n",
    "    cv=5,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "clf.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "    \n",
    "print(\"Best score\")\n",
    "print(clf.best_score_)\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(clf.grid_scores_)\n",
    "print()\n",
    "\n",
    "r_forest = RandomForestClassifier(bootstrap=clf.best_params_['bootstrap'], max_depth=clf.best_params_['max_depth'], max_features=clf.best_params_['max_features'], \n",
    "                            min_samples_leaf= clf.best_params_['min_samples_leaf'], min_samples_split= clf.best_params_['min_samples_split'],\n",
    "                           n_estimators = clf.best_params_['n_estimators'])\n",
    "    \n",
    "#r_forest = RandomForestClassifier(bootstrap=False, max_depth=25, max_features=23, min_samples_leaf= 3, min_samples_split= 2, n_estimators = 250)\n",
    "r_forest.fit(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), Y)\n",
    "Y_pred = r_forest.predict(Xt.reshape(Xt.shape[0], Xt.shape[1]*Xt.shape[2]))\n",
    "\n",
    "print(\"Accuracy Rate:\")\n",
    "print(metrics.accuracy_score(Yt, Y_pred))\n",
    "print(metrics.confusion_matrix(Yt, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(Yt, Y_pred, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(Yt, Y_pred, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(Yt, Y_pred)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(LABELS))\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
